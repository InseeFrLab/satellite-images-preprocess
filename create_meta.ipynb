{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astrovision.data.satellite_image import (\n",
    "    SatelliteImage,\n",
    ")\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.features import shapes\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from tqdm import tqdm\n",
    "from shapely import wkt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "import os\n",
    "\n",
    "fs = s3fs.S3FileSystem(\n",
    "    client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'},\n",
    "    key = os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "    secret = os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "    token = os.environ[\"AWS_SESSION_TOKEN\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "s3 = boto3.client(\"s3\",endpoint_url = 'https://'+'minio.lab.sspcloud.fr',\n",
    "                  aws_access_key_id= os.environ[\"AWS_ACCESS_KEY_ID\"], \n",
    "                  aws_secret_access_key= os.environ[\"AWS_SECRET_ACCESS_KEY\"], \n",
    "                  aws_session_token = os.environ[\"AWS_SESSION_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperer les métas d'un fichier\n",
    "\n",
    "bucket_name = \"projet-slums-detection\"\n",
    "file = 'data-raw/PLEIADES/GUYANE_brut/2024_meta/IMG_PHR1A_PMS-N_202402251358470_ORT_PHR_PRO_FOP760ad51fbb517_20241008193335117_1_1_R1C3.tif'\n",
    "\n",
    "existing_metadata = s3.head_object(Bucket=bucket_name, Key=file).get('Metadata', {})\n",
    "\n",
    "# les bb\n",
    "bb_image = existing_metadata['x-amz-meta-bounding_box']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def temporary_raster():\n",
    "    \"\"\"Context manager for handling temporary raster files safely.\"\"\"\n",
    "    temp = tempfile.NamedTemporaryFile(suffix=\".tif\", delete=False)\n",
    "    try:\n",
    "        temp.close()\n",
    "        yield temp.name\n",
    "    finally:\n",
    "        try:\n",
    "            os.unlink(temp.name)\n",
    "        except OSError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geojson_from_image(image: SatelliteImage) -> str:\n",
    "    \"\"\"\n",
    "    Creates a Geopandas from an image.\n",
    "    Args:\n",
    "        image: A SatelliteImage.\n",
    "    Returns:\n",
    "        A Geopandas representing the clusters with their respective labels.\n",
    "    \"\"\"\n",
    "    # Convert label to uint8\n",
    "    sum_bands = np.sum(image.array, axis=0)\n",
    "    binary_arr = np.where(sum_bands == 0, 0, 1)\n",
    "    binary_arr = binary_arr.astype(\"uint8\")\n",
    "\n",
    "    # Define the metadata for the raster image\n",
    "    metadata = {\n",
    "        \"driver\": \"GTiff\",\n",
    "        \"dtype\": \"uint8\",\n",
    "        \"count\": 1,\n",
    "        \"width\": binary_arr.shape[1],\n",
    "        \"height\": binary_arr.shape[0],\n",
    "        \"crs\": image.crs,\n",
    "        \"transform\": rasterio.transform.from_origin(\n",
    "            image.bounds[0], image.bounds[3], 0.5, 0.5\n",
    "        ),  # pixel size is 0.5m\n",
    "    }\n",
    "\n",
    "    # Use the context manager for temporary file handling\n",
    "    with temporary_raster() as temp_tif:\n",
    "        with rasterio.open(temp_tif, \"w+\", **metadata) as dst:\n",
    "            dst.write(binary_arr, 1)\n",
    "\n",
    "            # Process shapes within the same rasterio context\n",
    "            results = [\n",
    "                {\"properties\": {\"label\": int(v)}, \"geometry\": s}\n",
    "                for i, (s, v) in enumerate(shapes(binary_arr, mask=None, transform=dst.transform))\n",
    "                if v != 0  # Keep only the labels which are not 0\n",
    "            ]\n",
    "\n",
    "    # Create and return GeoDataFrame\n",
    "    if results:\n",
    "        return gpd.GeoDataFrame.from_features(results)\n",
    "    else:\n",
    "        return gpd.GeoDataFrame(columns=[\"geometry\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de fichiers : 1375\n"
     ]
    }
   ],
   "source": [
    "# Définition du bucket et du dossier à lister\n",
    "bucket_name = \"projet-slums-detection\"\n",
    "s3_directory = \"data-raw/PLEIADES/GUYANE_brut/2024/\"\n",
    "\n",
    "# Utilisation du paginator pour récupérer tous les fichiers\n",
    "paginator = s3.get_paginator(\"list_objects_v2\")\n",
    "pages = paginator.paginate(Bucket=bucket_name, Prefix=s3_directory)\n",
    "\n",
    "# Récupération de tous les fichiers\n",
    "file_keys = []\n",
    "for page in pages:\n",
    "    if \"Contents\" in page:\n",
    "        file_keys.extend(obj[\"Key\"] for obj in page[\"Contents\"])\n",
    "\n",
    "# Afficher le nombre total de fichiers trouvés\n",
    "print(f\"Nombre total de fichiers : {len(file_keys)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_keys = file_keys[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:01<00:00, 20.51s/it]\n"
     ]
    }
   ],
   "source": [
    "bounds = {}\n",
    "crs = {}\n",
    "polygon_images = {}\n",
    "df = pd.DataFrame(columns=['Filepath', 'Polygon'])\n",
    "error_files = []\n",
    "meta_files = []\n",
    "\n",
    "for file_key in tqdm(file_keys):\n",
    "    try :\n",
    "        image = SatelliteImage.from_raster(\n",
    "                            file_path=f\"/vsis3/{bucket_name}/{file_key}\",\n",
    "                        )\n",
    "    except:\n",
    "        error_files.append(file_key)\n",
    "    else:\n",
    "        bounds[file_key] = str(image.bounds)\n",
    "        crs[file_key] = str(image.crs)\n",
    "        polygon_image = create_geojson_from_image(image)\n",
    "        polygon_wkt = polygon_image.geometry[0].wkt\n",
    "        polygon_images[file_key] = polygon_wkt\n",
    "        df = pd.concat([df, pd.DataFrame([{'Filepath': file_key, 'Polygon': polygon_wkt}])], ignore_index=True)\n",
    "        meta_files.append(file_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_path = \"s3://projet-slums-detection/data-raw/PLEIADES/GUYANE_brut/polygones_images_brutes_2024.parquet\"\n",
    "with fs.open(s3_path, 'wb') as f:\n",
    "    df.to_parquet(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:18<00:00,  6.31s/it]\n"
     ]
    }
   ],
   "source": [
    "s3_directory_meta = s3_directory[:-1] + \"_metapoly/\" \n",
    "for file_key in tqdm(meta_files):\n",
    "    new_file = s3_directory_meta + file_key.split('/')[-1]\n",
    "\n",
    "    # Nouvelles métadonnées à ajouter\n",
    "    dt = s3.head_object(Bucket=bucket_name, Key=file_key).get('LastModified')\n",
    "    formatted_date = str(dt.replace(microsecond=300000).isoformat())\n",
    "\n",
    "    poly = wkt.loads(polygon_images[file_key])\n",
    "    simplified_polygon = poly.simplify(tolerance=5)\n",
    "    polygon_image_wkt = wkt.dumps(simplified_polygon, rounding_precision=2)\n",
    "\n",
    "    new_metadata = {\n",
    "        \"x-amz-meta-bounding_box\": bounds[file_key],\n",
    "        \"x-amz-meta-date\": formatted_date,\n",
    "        \"x-amz-meta-crs\": crs[file_key],\n",
    "        \"x-amz-meta-polygon\": polygon_image_wkt\n",
    "    }\n",
    "\n",
    "    # Étape 1 : Récupérer les métadonnées existantes\n",
    "    existing_metadata = s3.head_object(Bucket=bucket_name, Key=file_key).get('Metadata', {})\n",
    "\n",
    "    # Étape 2 : Ajouter les nouvelles métadonnées aux existantes\n",
    "    combined_metadata = existing_metadata.copy()\n",
    "    # Convertir les valeurs des tuples en chaînes\n",
    "    combined_metadata[\"x-amz-meta-bounding_box\"] = str(new_metadata[\"x-amz-meta-bounding_box\"])\n",
    "    combined_metadata[\"x-amz-meta-date\"] = str(new_metadata[\"x-amz-meta-date\"])\n",
    "    combined_metadata[\"x-amz-meta-crs\"] = str(new_metadata[\"x-amz-meta-crs\"])\n",
    "    combined_metadata[\"x-amz-meta-polygon\"] = str(new_metadata[\"x-amz-meta-polygon\"])\n",
    "\n",
    "    \n",
    "    # Étape 3 : Copier le fichier avec les nouvelles métadonnées\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket_name,\n",
    "        CopySource={'Bucket': bucket_name, 'Key': file_key},\n",
    "        Key=new_file,  # Ou remplacer avec le même nom pour écraser l'objet\n",
    "        Metadata=combined_metadata,\n",
    "        MetadataDirective='REPLACE'  # Remplacer les métadonnées existantes\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
